{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec7cb757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import subprocess, os\n",
    "from process_data import *\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from data.dataset import create_datasets\n",
    "from Generator.Generator import generator\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "bs = 100\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22376905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img=train_dataset[7]\n",
    "plt.imshow(img[0].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab6b655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16afc707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VAE\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # encoder part\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
    "        # decoder part\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h) # mu, log_var\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return F.sigmoid(self.fc6(h)) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x.view(-1, 784))\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n",
    "\n",
    "# build model\n",
    "vae = VAE(x_dim=784, h_dim1= 512, h_dim2=256, z_dim=2)\n",
    "if torch.cuda.is_available():\n",
    "    vae.cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "342a68e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(vae.parameters())\n",
    "# return reconstruction error + KL divergence losses\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdc99dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, log_var = vae(data)\n",
    "        loss = loss_function(recon_batch, data, mu, log_var)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edd55027",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test():\n",
    "    vae.eval()\n",
    "    test_loss= 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            data = data.cuda()\n",
    "            recon, mu, log_var = vae(data)\n",
    "            \n",
    "            # sum up batch loss\n",
    "            test_loss += loss_function(recon, data, mu, log_var).item()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e5698c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m51\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     test()\n",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 8\u001b[0m recon_batch, mu, log_var \u001b[38;5;241m=\u001b[39m \u001b[43mvae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(recon_batch, data, mu, log_var)\n\u001b[1;32m     11\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Documents/VAETI/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 34\u001b[0m     mu, log_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m784\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling(mu, log_var)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(z), mu, log_var\n",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36mVAE.encoder\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencoder\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 19\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[1;32m     20\u001b[0m     h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(h))\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc31(h), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc32(h)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch in range(1, 51):\n",
    "    train(epoch)\n",
    "    test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528e6438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
