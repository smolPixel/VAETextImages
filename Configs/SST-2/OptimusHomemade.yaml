---
dataset: "SST2"
computer: "labo"
dataset_size: 100
random_seed: 42
max_length: 0
algo: "OptimusHomemade"
#encoder: "GRU"
#decoder: "GRU"

#pretraining
pretraining: True
nb_epoch_pretraining: 10
annealing_strategy_pretraining: cyclical
ratios: [0.5, 0.25, 0.25]
pretraining_dataset: "Wikipedia"
pretraining_dataset_size: 50000


nb_epoch: 5
latent_size: 15
hidden_size: 1024
embedd_size: 300
num_layers: 1
batch_size: 8
dropout: 0.3
word_dropout: 0.3
x0: 15
strategy: ['Embedding']


device: 'cuda'

#
#base_model: "t5-base"
#pooling_strategy: "max" #mean
#lambda: 3
#denoise_percentage: 0.15
#fixed_reg_weight: 0
